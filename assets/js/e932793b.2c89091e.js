"use strict";(globalThis.webpackChunkbook=globalThis.webpackChunkbook||[]).push([[4645],{2877:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"bookTutorial":[{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-1/","label":"Module 1: The Robotic Nervous System (ROS 2)","docId":"book-tutorial/module-1/index","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-1/intro","label":"ROS 2 Foundations","docId":"book-tutorial/module-1/intro","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-1/python-integration","label":"Python Integration with Robotics","docId":"book-tutorial/module-1/python-integration","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-1/humanoid-urdf","label":"Humanoid Structure & URDF","docId":"book-tutorial/module-1/humanoid-urdf","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/","label":"Module 2: Digital Twin (Gazebo & Unity)","docId":"book-tutorial/module-2/index","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/intro","label":"Introduction to Digital Twin (Gazebo & Unity)","docId":"book-tutorial/module-2/intro","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/chapter1-physics-simulation","label":"Physics Simulation with Gazebo","docId":"book-tutorial/module-2/chapter1-physics-simulation","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/chapter2-unity-visualization","label":"Unity Visualization for Digital Twins","docId":"book-tutorial/module-2/chapter2-unity-visualization","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/chapter3-digital-twin-integration","label":"Digital Twin Integration and Applications","docId":"book-tutorial/module-2/chapter3-digital-twin-integration","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/data-model/","label":"Digital Twin Data Model","docId":"book-tutorial/module-2/data-model/data-model","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/integration/gazebo-unity-integration","label":"Gazebo-Unity Integration Guide","docId":"book-tutorial/module-2/integration/gazebo-unity-integration","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/physics/physics-simulation","label":"Physics Simulation with Gazebo","docId":"book-tutorial/module-2/physics/physics-simulation","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/quickstart/digital-twin-quickstart","label":"Quickstart Guide: Digital Twin Setup","docId":"book-tutorial/module-2/quickstart/digital-twin-quickstart","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/sensors/sensor-simulation","label":"Sensor Simulation in Gazebo","docId":"book-tutorial/module-2/sensors/sensor-simulation","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/unity/unity-visualization","label":"Unity Visualization for Digital Twins","docId":"book-tutorial/module-2/unity/unity-visualization","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-2/validation/validation-plan","label":"Validation Plan for Digital Twin Simulations","docId":"book-tutorial/module-2/validation/validation-plan","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","items":[{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-3/","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","docId":"book-tutorial/module-3/index","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-3/intro","label":"Chapter 1: Isaac Sim - Photorealistic Simulation and Synthetic Data","docId":"book-tutorial/module-3/intro","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-3/isaac-ros","label":"Chapter 2: Isaac ROS - Hardware-Accelerated Perception and VSLAM","docId":"book-tutorial/module-3/isaac-ros","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-3/nav2","label":"Chapter 3: Nav2 Navigation - Path Planning for Humanoid Movement","docId":"book-tutorial/module-3/nav2","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-4/","label":"Module 4: Vision-Language-Action (VLA) - The Cognitive Interface","docId":"book-tutorial/module-4/index","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-4/intro","label":"Vision-Language-Action (VLA) Overview","docId":"book-tutorial/module-4/intro","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-4/voice-to-action","label":"Voice-to-Action Pipeline","docId":"book-tutorial/module-4/voice-to-action","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-4/cognitive-planning","label":"Cognitive Planning with LLMs","docId":"book-tutorial/module-4/cognitive-planning","unlisted":false},{"type":"link","href":"/physical-ai-book/docs/book-tutorial/module-4/capstone-autonomous-humanoid","label":"Capstone \u2013 The Autonomous Humanoid","docId":"book-tutorial/module-4/capstone-autonomous-humanoid","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"01-01-01-introduction-ai-content-creation":{"id":"01-01-01-introduction-ai-content-creation","title":"Introduction to AI in Content Creation","description":"Understanding the fundamentals of AI tools for content generation in specification-driven book creation"},"01-02-01-specification-driven-development-principles":{"id":"01-02-01-specification-driven-development-principles","title":"Specification-Driven Development Principles","description":"Understanding specification-driven methodologies for book creation and content development"},"01-03-01-implementation-approach":{"id":"01-03-01-implementation-approach","title":"Implementation Approach for AI/Spec-Driven Book Creation","description":"Understanding the systematic approach to implementing AI-assisted content creation with specification-driven development"},"book-tutorial/module-1/humanoid-urdf":{"id":"book-tutorial/module-1/humanoid-urdf","title":"Humanoid Structure & URDF","description":"This chapter covers URDF (Unified Robot Description Format), the XML-based language used to describe robot models in ROS 2. You\'ll learn how to create humanoid robot models and connect them to the ROS 2 control system.","sidebar":"bookTutorial"},"book-tutorial/module-1/index":{"id":"book-tutorial/module-1/index","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"Welcome to Module 1, where we explore the foundation of modern robotics: ROS 2 (Robot Operating System 2). This module serves as your introduction to the robotic nervous system that enables communication, coordination, and control across robotic platforms.","sidebar":"bookTutorial"},"book-tutorial/module-1/intro":{"id":"book-tutorial/module-1/intro","title":"ROS 2 Foundations","description":"This chapter introduces you to the core concepts of ROS 2 (Robot Operating System 2), the framework that serves as the nervous system for modern robots. You\'ll learn about nodes, topics, services, and how they work together to create distributed robotic applications.","sidebar":"bookTutorial"},"book-tutorial/module-1/python-integration":{"id":"book-tutorial/module-1/python-integration","title":"Python Integration with Robotics","description":"This chapter explores how to integrate Python with ROS 2 using rclpy, the Python client library for ROS 2. Python is the dominant language in AI and machine learning, making it crucial for creating intelligent robotic systems.","sidebar":"bookTutorial"},"book-tutorial/module-2/chapter1-physics-simulation":{"id":"book-tutorial/module-2/chapter1-physics-simulation","title":"Physics Simulation with Gazebo","description":"Gazebo is a powerful physics simulation environment that provides realistic robot simulation capabilities. It\'s widely used in robotics research and development for testing and validation purposes.","sidebar":"bookTutorial"},"book-tutorial/module-2/chapter2-unity-visualization":{"id":"book-tutorial/module-2/chapter2-unity-visualization","title":"Unity Visualization for Digital Twins","description":"Unity is a powerful 3D development platform that can be integrated with robotics simulation to create advanced visualization and user interfaces. For digital twins, Unity provides photorealistic rendering and immersive visualization capabilities.","sidebar":"bookTutorial"},"book-tutorial/module-2/chapter3-digital-twin-integration":{"id":"book-tutorial/module-2/chapter3-digital-twin-integration","title":"Digital Twin Integration and Applications","description":"Now that we understand the individual components of digital twins, let\'s explore how to integrate them into comprehensive systems and apply them to real-world robotics problems, particularly for humanoid robots.","sidebar":"bookTutorial"},"book-tutorial/module-2/data-model/data-model":{"id":"book-tutorial/module-2/data-model/data-model","title":"Digital Twin Data Model","description":"This chapter defines the data structures and schemas used in the Gazebo-Unity digital twin system. Understanding these data models is crucial for developing robust simulation and visualization components.","sidebar":"bookTutorial"},"book-tutorial/module-2/index":{"id":"book-tutorial/module-2/index","title":"Module 2: Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2, where we explore the creation of physics-based digital twins using Gazebo for simulation and Unity for high-fidelity visualization. This module covers the integration of both platforms to simulate realistic environments and robotic systems.","sidebar":"bookTutorial"},"book-tutorial/module-2/integration/gazebo-unity-integration":{"id":"book-tutorial/module-2/integration/gazebo-unity-integration","title":"Gazebo-Unity Integration Guide","description":"Integrating Gazebo and Unity creates a powerful digital twin system combining accurate physics simulation with high-fidelity visualization. This chapter details the architecture, communication protocols, and implementation strategies for seamless integration.","sidebar":"bookTutorial"},"book-tutorial/module-2/intro":{"id":"book-tutorial/module-2/intro","title":"Introduction to Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2, where we explore the concept of Digital Twins in robotics. A digital twin is a virtual replica of a physical robot that allows engineers to simulate, analyze, and optimize robot behavior in a safe, virtual environment before deploying to the real world.","sidebar":"bookTutorial"},"book-tutorial/module-2/physics/physics-simulation":{"id":"book-tutorial/module-2/physics/physics-simulation","title":"Physics Simulation with Gazebo","description":"Gazebo is a powerful physics-based simulation environment that enables realistic modeling of robots and their environments. This chapter covers the fundamentals of setting up and configuring physics simulations.","sidebar":"bookTutorial"},"book-tutorial/module-2/quickstart/digital-twin-quickstart":{"id":"book-tutorial/module-2/quickstart/digital-twin-quickstart","title":"Quickstart Guide: Digital Twin Setup","description":"This guide will help you set up your first Gazebo-Unity digital twin system. Follow these steps to create a basic integrated environment for physics simulation and high-fidelity visualization.","sidebar":"bookTutorial"},"book-tutorial/module-2/sensors/sensor-simulation":{"id":"book-tutorial/module-2/sensors/sensor-simulation","title":"Sensor Simulation in Gazebo","description":"Accurate sensor simulation is critical for creating realistic digital twins. This chapter covers the simulation of LiDAR, depth cameras, and IMU sensors in Gazebo.","sidebar":"bookTutorial"},"book-tutorial/module-2/unity/unity-visualization":{"id":"book-tutorial/module-2/unity/unity-visualization","title":"Unity Visualization for Digital Twins","description":"Unity provides high-fidelity visualization capabilities that complement Gazebo\'s physics simulation. This chapter covers setting up Unity for digital twin visualization and best practices for creating immersive environments.","sidebar":"bookTutorial"},"book-tutorial/module-2/validation/validation-plan":{"id":"book-tutorial/module-2/validation/validation-plan","title":"Validation Plan for Digital Twin Simulations","description":"This chapter outlines the comprehensive validation strategy for ensuring the accuracy, reliability, and performance of Gazebo-Unity digital twin simulations. The validation plan encompasses physics accuracy, sensor fidelity, system integration, and performance benchmarks.","sidebar":"bookTutorial"},"book-tutorial/module-3/index":{"id":"book-tutorial/module-3/index","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"Welcome to Module 3, where we explore the advanced perception, simulation, and navigation intelligence that forms the \\"brain\\" of modern humanoid robots. This module focuses on NVIDIA Isaac technologies that enable robots to perceive their environment, plan intelligent actions, and navigate complex spaces.","sidebar":"bookTutorial"},"book-tutorial/module-3/intro":{"id":"book-tutorial/module-3/intro","title":"Chapter 1: Isaac Sim - Photorealistic Simulation and Synthetic Data","description":"Concept","sidebar":"bookTutorial"},"book-tutorial/module-3/isaac-ros":{"id":"book-tutorial/module-3/isaac-ros","title":"Chapter 2: Isaac ROS - Hardware-Accelerated Perception and VSLAM","description":"Concept","sidebar":"bookTutorial"},"book-tutorial/module-3/nav2":{"id":"book-tutorial/module-3/nav2","title":"Chapter 3: Nav2 Navigation - Path Planning for Humanoid Movement","description":"Concept","sidebar":"bookTutorial"},"book-tutorial/module-4/capstone-autonomous-humanoid":{"id":"book-tutorial/module-4/capstone-autonomous-humanoid","title":"Capstone \u2013 The Autonomous Humanoid","description":"The Autonomous Humanoid represents the integration of all Vision-Language-Action (VLA) capabilities into a cohesive system that can receive voice commands, plan complex behaviors, navigate environments, identify objects, and manipulate them autonomously. This capstone demonstrates the complete VLA paradigm in action.","sidebar":"bookTutorial"},"book-tutorial/module-4/cognitive-planning":{"id":"book-tutorial/module-4/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Large Language Models (LLMs) serve as the cognitive planners in Vision-Language-Action (VLA) systems, bridging the gap between high-level natural language commands and executable robotic actions. This chapter explores how LLMs function as high-level cognitive planners rather than low-level controllers, enabling robots to understand and execute complex tasks through natural language interaction.","sidebar":"bookTutorial"},"book-tutorial/module-4/index":{"id":"book-tutorial/module-4/index","title":"Module 4: Vision-Language-Action (VLA) - The Cognitive Interface","description":"Welcome to Module 4, where we explore the cutting-edge convergence of artificial intelligence and robotics through the Vision-Language-Action (VLA) paradigm. This module focuses on how humanoid robots can understand natural language commands, perceive their environment visually, and execute complex physical actions to achieve high-level goals.","sidebar":"bookTutorial"},"book-tutorial/module-4/intro":{"id":"book-tutorial/module-4/intro","title":"Vision-Language-Action (VLA) Overview","description":"The Vision-Language-Action (VLA) paradigm represents a fundamental shift in how we approach human-robot interaction, enabling robots to understand natural language commands, perceive their environment visually, and execute complex physical actions to achieve high-level goals. This chapter introduces the foundational concepts that make this convergence possible.","sidebar":"bookTutorial"},"book-tutorial/module-4/voice-to-action":{"id":"book-tutorial/module-4/voice-to-action","title":"Voice-to-Action Pipeline","description":"The Voice-to-Action pipeline represents the complete transformation process that converts spoken human commands into executable robotic actions. This pipeline is the cornerstone of natural human-robot interaction, enabling robots to understand and respond to verbal instructions in human environments.","sidebar":"bookTutorial"},"digital-twin-architecture":{"id":"digital-twin-architecture","title":"Digital Twin Architecture for Humanoid Robotics","description":"A digital twin architecture for humanoid robotics combines accurate physics simulation, high-fidelity visualization, and real-time data synchronization to create a comprehensive virtual representation of physical humanoid robots. This chapter details the architecture, communication protocols, and implementation strategies for building effective digital twin systems."},"gazebo-physics-simulation":{"id":"gazebo-physics-simulation","title":"Gazebo Physics Simulation","description":"Physics simulation is fundamental to creating realistic digital twins for humanoid robotics. Gazebo provides a robust physics engine that accurately models the dynamics, collisions, and interactions that humanoid robots experience in real-world environments."},"gazebo-unity-integration/conclusion":{"id":"gazebo-unity-integration/conclusion","title":"Best Practices Summary: Gazebo-Unity Integration for Educational Robotics","description":"Executive Summary"},"gazebo-unity-integration/educational-patterns/index":{"id":"gazebo-unity-integration/educational-patterns/index","title":"Educational Patterns for Gazebo-Unity Integration","description":"Overview"},"gazebo-unity-integration/folder-structure":{"id":"gazebo-unity-integration/folder-structure","title":"Recommended Folder Structure for Docusaurus Books","description":"Overview"},"gazebo-unity-integration/index":{"id":"gazebo-unity-integration/index","title":"Gazebo-Unity Integration for Robotics Simulation","description":"Overview"},"gazebo-unity-integration/sensor-simulation/index":{"id":"gazebo-unity-integration/sensor-simulation/index","title":"Sensor Simulation Across Gazebo and Unity","description":"Overview"},"intro":{"id":"intro","title":"Introduction to AI/Spec-Driven Book Creation","description":"Welcome to AI/Spec-Driven Book Creation, a comprehensive guide to developing educational content using artificial intelligence and specification-driven development methodologies."},"module-1-ros2/humanoid-urdf":{"id":"module-1-ros2/humanoid-urdf","title":"Humanoid Structure & URDF","description":"This chapter covers URDF (Unified Robot Description Format), the XML-based language used to describe robot models in ROS 2. You\'ll learn how to create humanoid robot models and connect them to the ROS 2 control system."},"module-1-ros2/index":{"id":"module-1-ros2/index","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"Welcome to Module 1, where we explore the foundation of modern robotics: ROS 2 (Robot Operating System 2). This module serves as your introduction to the robotic nervous system that enables communication, coordination, and control across robotic platforms."},"module-1-ros2/intro":{"id":"module-1-ros2/intro","title":"ROS 2 Foundations","description":"This chapter introduces you to the core concepts of ROS 2 (Robot Operating System 2), the framework that serves as the nervous system for modern robots. You\'ll learn about nodes, topics, services, and how they work together to create distributed robotic applications."},"module-1-ros2/python-integration":{"id":"module-1-ros2/python-integration","title":"Python Integration with Robotics","description":"This chapter explores how to integrate Python with ROS 2 using rclpy, the Python client library for ROS 2. Python is the dominant language in AI and machine learning, making it crucial for creating intelligent robotic systems."},"module-3-ai-robot-brain/index":{"id":"module-3-ai-robot-brain/index","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"Welcome to Module 3, where we explore the advanced perception, simulation, and navigation intelligence that forms the \\"brain\\" of modern humanoid robots. This module focuses on NVIDIA Isaac technologies that enable robots to perceive their environment, plan intelligent actions, and navigate complex spaces."},"module-3-ai-robot-brain/intro":{"id":"module-3-ai-robot-brain/intro","title":"Chapter 1: Isaac Sim - Photorealistic Simulation and Synthetic Data","description":"Concept"},"module-3-ai-robot-brain/isaac-ros":{"id":"module-3-ai-robot-brain/isaac-ros","title":"Chapter 2: Isaac ROS - Hardware-Accelerated Perception and VSLAM","description":"Concept"},"module-3-ai-robot-brain/nav2":{"id":"module-3-ai-robot-brain/nav2","title":"Chapter 3: Nav2 Navigation - Path Planning for Humanoid Movement","description":"Concept"},"module-4-vla/capstone-autonomous-humanoid":{"id":"module-4-vla/capstone-autonomous-humanoid","title":"Capstone \u2013 The Autonomous Humanoid","description":"The Autonomous Humanoid represents the integration of all Vision-Language-Action (VLA) capabilities into a cohesive system that can receive voice commands, plan complex behaviors, navigate environments, identify objects, and manipulate them autonomously. This capstone demonstrates the complete VLA paradigm in action."},"module-4-vla/cognitive-planning":{"id":"module-4-vla/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Large Language Models (LLMs) serve as the cognitive planners in Vision-Language-Action (VLA) systems, bridging the gap between high-level natural language commands and executable robotic actions. This chapter explores how LLMs function as high-level cognitive planners rather than low-level controllers, enabling robots to understand and execute complex tasks through natural language interaction."},"module-4-vla/index":{"id":"module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA) - The Cognitive Interface","description":"Welcome to Module 4, where we explore the cutting-edge convergence of artificial intelligence and robotics through the Vision-Language-Action (VLA) paradigm. This module focuses on how humanoid robots can understand natural language commands, perceive their environment visually, and execute complex physical actions to achieve high-level goals."},"module-4-vla/intro":{"id":"module-4-vla/intro","title":"Vision-Language-Action (VLA) Overview","description":"The Vision-Language-Action (VLA) paradigm represents a fundamental shift in how we approach human-robot interaction, enabling robots to understand natural language commands, perceive their environment visually, and execute complex physical actions to achieve high-level goals. This chapter introduces the foundational concepts that make this convergence possible."},"module-4-vla/voice-to-action":{"id":"module-4-vla/voice-to-action","title":"Voice-to-Action Pipeline","description":"The Voice-to-Action pipeline represents the complete transformation process that converts spoken human commands into executable robotic actions. This pipeline is the cornerstone of natural human-robot interaction, enabling robots to understand and respond to verbal instructions in human environments."},"module2/chapter1-physics-simulation":{"id":"module2/chapter1-physics-simulation","title":"Physics Simulation with Gazebo","description":"Gazebo is a powerful physics simulation environment that provides realistic robot simulation capabilities. It\'s widely used in robotics research and development for testing and validation purposes."},"module2/chapter2-unity-visualization":{"id":"module2/chapter2-unity-visualization","title":"Unity Visualization for Digital Twins","description":"Unity is a powerful 3D development platform that can be integrated with robotics simulation to create advanced visualization and user interfaces. For digital twins, Unity provides photorealistic rendering and immersive visualization capabilities."},"module2/chapter3-digital-twin-integration":{"id":"module2/chapter3-digital-twin-integration","title":"Digital Twin Integration and Applications","description":"Now that we understand the individual components of digital twins, let\'s explore how to integrate them into comprehensive systems and apply them to real-world robotics problems, particularly for humanoid robots."},"module2/data-model/data-model":{"id":"module2/data-model/data-model","title":"Digital Twin Data Model","description":"This chapter defines the data structures and schemas used in the Gazebo-Unity digital twin system. Understanding these data models is crucial for developing robust simulation and visualization components."},"module2/index":{"id":"module2/index","title":"Module 2: Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2, where we explore the creation of physics-based digital twins using Gazebo for simulation and Unity for high-fidelity visualization. This module covers the integration of both platforms to simulate realistic environments and robotic systems."},"module2/integration/gazebo-unity-integration":{"id":"module2/integration/gazebo-unity-integration","title":"Gazebo-Unity Integration Guide","description":"Integrating Gazebo and Unity creates a powerful digital twin system combining accurate physics simulation with high-fidelity visualization. This chapter details the architecture, communication protocols, and implementation strategies for seamless integration."},"module2/intro":{"id":"module2/intro","title":"Introduction to Digital Twin (Gazebo & Unity)","description":"Welcome to Module 2, where we explore the concept of Digital Twins in robotics. A digital twin is a virtual replica of a physical robot that allows engineers to simulate, analyze, and optimize robot behavior in a safe, virtual environment before deploying to the real world."},"module2/physics/physics-simulation":{"id":"module2/physics/physics-simulation","title":"Physics Simulation with Gazebo","description":"Gazebo is a powerful physics-based simulation environment that enables realistic modeling of robots and their environments. This chapter covers the fundamentals of setting up and configuring physics simulations."},"module2/quickstart/digital-twin-quickstart":{"id":"module2/quickstart/digital-twin-quickstart","title":"Quickstart Guide: Digital Twin Setup","description":"This guide will help you set up your first Gazebo-Unity digital twin system. Follow these steps to create a basic integrated environment for physics simulation and high-fidelity visualization."},"module2/sensors/sensor-simulation":{"id":"module2/sensors/sensor-simulation","title":"Sensor Simulation in Gazebo","description":"Accurate sensor simulation is critical for creating realistic digital twins. This chapter covers the simulation of LiDAR, depth cameras, and IMU sensors in Gazebo."},"module2/unity/unity-visualization":{"id":"module2/unity/unity-visualization","title":"Unity Visualization for Digital Twins","description":"Unity provides high-fidelity visualization capabilities that complement Gazebo\'s physics simulation. This chapter covers setting up Unity for digital twin visualization and best practices for creating immersive environments."},"module2/validation/validation-plan":{"id":"module2/validation/validation-plan","title":"Validation Plan for Digital Twin Simulations","description":"This chapter outlines the comprehensive validation strategy for ensuring the accuracy, reliability, and performance of Gazebo-Unity digital twin simulations. The validation plan encompasses physics accuracy, sensor fidelity, system integration, and performance benchmarks."},"sensor-simulation":{"id":"sensor-simulation","title":"Sensor Simulation for Humanoid Robots","description":"Accurate sensor simulation is critical for creating realistic digital twins of humanoid robots. This chapter covers the simulation of LiDAR, depth cameras, IMU sensors, and other sensors specifically relevant for humanoid robot perception and navigation in complex environments."},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template."},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed..."},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack)."},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features."},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs."},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French."},"unity-human-robot-interaction":{"id":"unity-human-robot-interaction","title":"Unity Human-Robot Interaction","description":"Unity provides high-fidelity visualization and interaction capabilities that complement physics simulation for creating immersive human-robot interaction experiences. This chapter covers setting up Unity for human-robot interaction scenarios and best practices for creating engaging interfaces."}}}}')}}]);